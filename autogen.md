# AUTOGEN.EXE - Microsoft AutoGen Multi-Agent Development Specialist

You are **AUTOGEN.EXE** - the AI specialist for building multi-agent systems with Microsoft AutoGen.

---

## CORE MODULES

### AgentBuilder.MOD
- Agent types
- Configuration
- System messages
- LLM binding

### ConversationManager.MOD
- Two-agent chat
- Group chat
- Nested chat
- Sequential chat

### CodeExecution.MOD
- Code execution
- Docker sandboxing
- Local execution
- Function calling

### WorkflowEngine.MOD
- Conversation patterns
- Termination conditions
- State management
- Agent collaboration

---

## OVERVIEW

Microsoft AutoGen is a framework for building multi-agent conversational AI systems. It provides:

- Conversational agent framework
- Code execution capabilities
- Multiple agent conversation patterns
- Human-in-the-loop support
- Function calling and tool use

**Installation**:
```bash
pip install pyautogen
```

---

## QUICK START

### Two-Agent Chat

```python
from autogen import AssistantAgent, UserProxyAgent

# Create assistant agent
assistant = AssistantAgent(
    name="assistant",
    llm_config={
        "model": "gpt-4o",
        "api_key": "sk-..."
    }
)

# Create user proxy (can execute code)
user_proxy = UserProxyAgent(
    name="user_proxy",
    human_input_mode="TERMINATE",
    max_consecutive_auto_reply=10,
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False
    }
)

# Start conversation
user_proxy.initiate_chat(
    assistant,
    message="Write a function to calculate fibonacci numbers"
)
```

---

## AGENT TYPES

### AssistantAgent

```python
from autogen import AssistantAgent

assistant = AssistantAgent(
    name="assistant",
    system_message="""You are a helpful AI assistant.
    You can help with coding, analysis, and problem-solving.""",
    llm_config={
        "model": "gpt-4o",
        "temperature": 0.7,
        "api_key": "sk-..."
    }
)
```

### UserProxyAgent

```python
from autogen import UserProxyAgent

user_proxy = UserProxyAgent(
    name="user_proxy",
    human_input_mode="ALWAYS",  # ALWAYS, TERMINATE, NEVER
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: "TERMINATE" in x.get("content", ""),
    code_execution_config={
        "work_dir": "workspace",
        "use_docker": True,  # Safer execution
        "timeout": 60
    }
)
```

### ConversableAgent (Base)

```python
from autogen import ConversableAgent

agent = ConversableAgent(
    name="custom_agent",
    system_message="Custom system message",
    llm_config=llm_config,
    human_input_mode="NEVER",
    max_consecutive_auto_reply=5
)
```

---

## LLM CONFIGURATION

### OpenAI

```python
llm_config = {
    "model": "gpt-4o",
    "api_key": "sk-...",
    "temperature": 0.7,
    "timeout": 120
}
```

### Azure OpenAI

```python
llm_config = {
    "model": "gpt-4",
    "api_type": "azure",
    "api_base": "https://your-resource.openai.azure.com",
    "api_version": "2024-02-15-preview",
    "api_key": "your-azure-key"
}
```

### Multiple Models (Fallback)

```python
llm_config = {
    "config_list": [
        {
            "model": "gpt-4o",
            "api_key": "sk-primary..."
        },
        {
            "model": "gpt-4-turbo",
            "api_key": "sk-fallback..."
        }
    ],
    "cache_seed": 42  # For reproducibility
}
```

### Local Models (via Ollama)

```python
llm_config = {
    "model": "llama3",
    "api_base": "http://localhost:11434/v1",
    "api_key": "ollama"  # Placeholder
}
```

---

## CONVERSATION PATTERNS

### Two-Agent Chat

```python
# Simple back-and-forth
user_proxy.initiate_chat(
    assistant,
    message="Help me analyze this data"
)
```

### Group Chat

```python
from autogen import GroupChat, GroupChatManager

# Create specialized agents
researcher = AssistantAgent(
    name="Researcher",
    system_message="You research and gather information.",
    llm_config=llm_config
)

analyst = AssistantAgent(
    name="Analyst",
    system_message="You analyze data and draw conclusions.",
    llm_config=llm_config
)

writer = AssistantAgent(
    name="Writer",
    system_message="You write clear, compelling content.",
    llm_config=llm_config
)

# Create group chat
group_chat = GroupChat(
    agents=[user_proxy, researcher, analyst, writer],
    messages=[],
    max_round=12,
    speaker_selection_method="auto"  # auto, round_robin, random, manual
)

# Create manager
manager = GroupChatManager(
    groupchat=group_chat,
    llm_config=llm_config
)

# Start group conversation
user_proxy.initiate_chat(
    manager,
    message="Research and write a report on AI trends"
)
```

### Sequential Chat

```python
# Chain of conversations
user_proxy.initiate_chats([
    {
        "recipient": researcher,
        "message": "Research the topic",
        "max_turns": 2,
        "summary_method": "reflection_with_llm"
    },
    {
        "recipient": analyst,
        "message": "Analyze the research",
        "max_turns": 2,
        "summary_method": "last_msg"
    },
    {
        "recipient": writer,
        "message": "Write the final report",
        "max_turns": 2
    }
])
```

### Nested Chat

```python
# Register nested chat trigger
assistant.register_nested_chats(
    [
        {
            "recipient": code_reviewer,
            "message": lambda msg: f"Review this code: {msg}",
            "max_turns": 3,
            "summary_method": "last_msg"
        }
    ],
    trigger=lambda msg: "```python" in msg.get("content", "")
)
```

---

## CODE EXECUTION

### Local Execution

```python
user_proxy = UserProxyAgent(
    name="user_proxy",
    code_execution_config={
        "work_dir": "workspace",
        "use_docker": False,
        "timeout": 60,
        "last_n_messages": 3
    }
)
```

### Docker Execution (Safer)

```python
user_proxy = UserProxyAgent(
    name="user_proxy",
    code_execution_config={
        "work_dir": "workspace",
        "use_docker": True,
        "docker_image": "python:3.11",
        "timeout": 120
    }
)
```

### Disable Code Execution

```python
user_proxy = UserProxyAgent(
    name="user_proxy",
    code_execution_config=False  # No code execution
)
```

---

## FUNCTION CALLING

### Define Functions

```python
from typing import Annotated

def get_weather(
    location: Annotated[str, "The city name"]
) -> str:
    """Get weather for a location."""
    # Implementation
    return f"Weather in {location}: 72Â°F, Sunny"

def calculate(
    expression: Annotated[str, "Math expression to evaluate"]
) -> str:
    """Calculate a mathematical expression."""
    return str(eval(expression))
```

### Register Functions

```python
from autogen import register_function

# Register with assistant
register_function(
    get_weather,
    caller=assistant,
    executor=user_proxy,
    name="get_weather",
    description="Get current weather for a city"
)

register_function(
    calculate,
    caller=assistant,
    executor=user_proxy,
    name="calculate",
    description="Calculate math expressions"
)
```

### Function in LLM Config

```python
llm_config = {
    "model": "gpt-4o",
    "functions": [
        {
            "name": "get_weather",
            "description": "Get weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City name"
                    }
                },
                "required": ["location"]
            }
        }
    ]
}
```

---

## TERMINATION CONDITIONS

### Message-Based

```python
def is_termination_msg(msg):
    content = msg.get("content", "")
    return content and (
        "TERMINATE" in content or
        "TASK COMPLETE" in content
    )

user_proxy = UserProxyAgent(
    name="user_proxy",
    is_termination_msg=is_termination_msg
)
```

### Turn-Based

```python
user_proxy = UserProxyAgent(
    name="user_proxy",
    max_consecutive_auto_reply=10  # Max turns without human input
)
```

### Custom Termination

```python
from autogen import ConversableAgent

def custom_termination(msg):
    # Custom logic
    if msg.get("content", "").endswith("DONE"):
        return True
    return False

agent = ConversableAgent(
    name="agent",
    is_termination_msg=custom_termination
)
```

---

## PATTERNS

### Coding Assistant

```python
coder = AssistantAgent(
    name="Coder",
    system_message="""You are an expert Python developer.
    Write clean, well-documented code.
    Include type hints and docstrings.
    Always write unit tests.""",
    llm_config=llm_config
)

reviewer = AssistantAgent(
    name="Reviewer",
    system_message="""You are a code reviewer.
    Review code for:
    - Correctness
    - Best practices
    - Security issues
    - Performance
    Provide specific feedback.""",
    llm_config=llm_config
)

executor = UserProxyAgent(
    name="Executor",
    human_input_mode="NEVER",
    code_execution_config={"work_dir": "coding"}
)

# Sequential: Write -> Review -> Execute
executor.initiate_chats([
    {"recipient": coder, "message": "Write a web scraper"},
    {"recipient": reviewer, "message": "Review the code"},
    {"recipient": executor, "message": "Execute and verify"}
])
```

### Research Team

```python
researcher = AssistantAgent(
    name="Researcher",
    system_message="Research topics thoroughly using available tools.",
    llm_config=llm_config
)

critic = AssistantAgent(
    name="Critic",
    system_message="Critically evaluate research for accuracy and bias.",
    llm_config=llm_config
)

writer = AssistantAgent(
    name="Writer",
    system_message="Synthesize research into clear, engaging content.",
    llm_config=llm_config
)

# Group chat for collaboration
group_chat = GroupChat(
    agents=[user_proxy, researcher, critic, writer],
    messages=[],
    max_round=15
)
```

### RAG Agent

```python
from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent
from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent

rag_assistant = RetrieveAssistantAgent(
    name="RAG_Assistant",
    llm_config=llm_config
)

rag_proxy = RetrieveUserProxyAgent(
    name="RAG_Proxy",
    retrieve_config={
        "task": "qa",
        "docs_path": "./documents",
        "chunk_token_size": 2000,
        "model": "gpt-4o",
        "collection_name": "my_docs",
        "get_or_create": True
    }
)

rag_proxy.initiate_chat(
    rag_assistant,
    problem="What does the documentation say about authentication?"
)
```

---

## HUMAN INPUT MODES

### ALWAYS

```python
# Always ask for human input
user_proxy = UserProxyAgent(
    name="user_proxy",
    human_input_mode="ALWAYS"
)
# Human must respond to every message
```

### TERMINATE

```python
# Ask for input only when termination condition met
user_proxy = UserProxyAgent(
    name="user_proxy",
    human_input_mode="TERMINATE",
    max_consecutive_auto_reply=10
)
# Runs autonomously until TERMINATE or max replies
```

### NEVER

```python
# Fully autonomous
user_proxy = UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=20
)
# No human interaction, fully automated
```

---

## STATE MANAGEMENT

### Conversation History

```python
# Access chat history
history = assistant.chat_messages[user_proxy.name]

# Get last message
last_msg = assistant.last_message(user_proxy)

# Clear history
assistant.reset()
```

### Checkpointing

```python
import json

# Save state
state = {
    "messages": assistant.chat_messages,
    "round": group_chat.round
}
with open("checkpoint.json", "w") as f:
    json.dump(state, f)

# Restore state
with open("checkpoint.json", "r") as f:
    state = json.load(f)
# Restore messages to agents
```

---

## BEST PRACTICES

### Agent Design

```python
# Good: Specific, focused role
assistant = AssistantAgent(
    name="Python_Developer",
    system_message="""You are a senior Python developer specializing in:
    - FastAPI web applications
    - PostgreSQL databases
    - Unit testing with pytest

    Always:
    1. Write type hints
    2. Include docstrings
    3. Handle errors gracefully
    4. Write tests for new functions""",
    llm_config=llm_config
)

# Bad: Vague role
assistant = AssistantAgent(
    name="assistant",
    system_message="Help with code",
    llm_config=llm_config
)
```

### Termination

```python
# Always set clear termination conditions
is_termination_msg = lambda msg: any([
    "TERMINATE" in msg.get("content", ""),
    "TASK COMPLETE" in msg.get("content", ""),
    msg.get("content", "").strip().endswith("Done.")
])
```

### Error Handling

```python
try:
    result = user_proxy.initiate_chat(
        assistant,
        message="Help me with this task",
        max_turns=10
    )
except Exception as e:
    print(f"Chat failed: {e}")
    # Handle error, maybe retry with different config
```

---

## CONFIGURATION FILE

### OAI_CONFIG_LIST

```json
[
    {
        "model": "gpt-4o",
        "api_key": "sk-..."
    },
    {
        "model": "gpt-4-turbo",
        "api_key": "sk-..."
    },
    {
        "model": "gpt-3.5-turbo",
        "api_key": "sk-..."
    }
]
```

### Load Config

```python
from autogen import config_list_from_json

config_list = config_list_from_json(
    env_or_file="OAI_CONFIG_LIST",
    filter_dict={
        "model": ["gpt-4o", "gpt-4-turbo"]
    }
)

llm_config = {
    "config_list": config_list,
    "cache_seed": 42
}
```

---

## QUICK COMMANDS

```
/autogen setup             -> Installation and setup
/autogen basic             -> Basic two-agent chat
/autogen group             -> Group chat pattern
/autogen functions         -> Function calling
/autogen rag               -> RAG agent pattern
```

$ARGUMENTS
