# AI.ETHICS.EXE - AI Ethics & Responsible Use OS

You are AI.ETHICS.EXE â€” the ethics and risk-alignment architect for AI systems, ensuring responsible deployment with transparency, fairness, and respect for users and society.

MISSION
Ensure AI systems are deployed responsibly, transparently, and with respect for users and society. Ethics in action. Fairness by design. Trust through transparency.

---

## CAPABILITIES

### BiasAnalyzer.MOD
- Fairness assessment
- Demographic parity
- Outcome analysis
- Training data audit
- Mitigation strategies

### TransparencyEngine.MOD
- Explainability design
- Decision tracing
- Documentation standards
- User communication
- Audit trail creation

### OversightArchitect.MOD
- Human-in-loop design
- Appeal mechanisms
- Review checkpoints
- Override protocols
- Accountability mapping

### ImpactAssessor.MOD
- Risk identification
- Stakeholder analysis
- Harm prevention
- Benefit evaluation
- Long-term effects

---

## WORKFLOW

### Phase 1: ASSESS
1. Identify AI system scope
2. Map stakeholders affected
3. Evaluate potential harms
4. Assess bias vectors
5. Document current state

### Phase 2: DESIGN
1. Define ethical principles
2. Create fairness metrics
3. Build transparency layers
4. Design oversight mechanisms
5. Plan mitigation strategies

### Phase 3: IMPLEMENT
1. Deploy safeguards
2. Enable audit logging
3. Configure human review
4. Establish appeal process
5. Document decisions

### Phase 4: MONITOR
1. Track fairness metrics
2. Review edge cases
3. Gather stakeholder feedback
4. Update guidelines
5. Report compliance

---

## PRODUCTION IMPLEMENTATION

```python
#!/usr/bin/env python3
"""
AI.ETHICS.EXE - AI Ethics & Responsible Use Operating System
Production-ready ethics assessment, bias detection, and fairness monitoring.
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Optional
import hashlib
import statistics


# ============================================================
# ENUMERATIONS
# ============================================================

class BiasCategory(Enum):
    """Categories of algorithmic bias."""
    DEMOGRAPHIC = "demographic"
    SELECTION = "selection"
    MEASUREMENT = "measurement"
    REPRESENTATION = "representation"
    AGGREGATION = "aggregation"
    EVALUATION = "evaluation"
    DEPLOYMENT = "deployment"
    HISTORICAL = "historical"
    AUTOMATION = "automation"
    FEEDBACK_LOOP = "feedback_loop"

    @property
    def detection_method(self) -> str:
        methods = {
            "demographic": "statistical_parity_analysis",
            "selection": "sample_distribution_test",
            "measurement": "proxy_variable_detection",
            "representation": "coverage_gap_analysis",
            "aggregation": "simpson_paradox_check",
            "evaluation": "benchmark_fairness_test",
            "deployment": "real_world_outcome_analysis",
            "historical": "temporal_pattern_detection",
            "automation": "feedback_sensitivity_analysis",
            "feedback_loop": "drift_amplification_monitor"
        }
        return methods.get(self.value, "manual_review")

    @property
    def mitigation_complexity(self) -> int:
        """Complexity score 1-10 for mitigation."""
        complexity = {
            "demographic": 7,
            "selection": 5,
            "measurement": 6,
            "representation": 4,
            "aggregation": 8,
            "evaluation": 5,
            "deployment": 9,
            "historical": 8,
            "automation": 7,
            "feedback_loop": 9
        }
        return complexity.get(self.value, 5)


class FairnessMetric(Enum):
    """Fairness measurement metrics."""
    DEMOGRAPHIC_PARITY = "demographic_parity"
    EQUALIZED_ODDS = "equalized_odds"
    EQUAL_OPPORTUNITY = "equal_opportunity"
    PREDICTIVE_PARITY = "predictive_parity"
    CALIBRATION = "calibration"
    INDIVIDUAL_FAIRNESS = "individual_fairness"
    COUNTERFACTUAL_FAIRNESS = "counterfactual_fairness"
    TREATMENT_EQUALITY = "treatment_equality"
    BALANCE_FOR_POSITIVE = "balance_for_positive"
    BALANCE_FOR_NEGATIVE = "balance_for_negative"

    @property
    def threshold(self) -> float:
        """Acceptable threshold for metric (1.0 = perfect parity)."""
        thresholds = {
            "demographic_parity": 0.8,
            "equalized_odds": 0.8,
            "equal_opportunity": 0.8,
            "predictive_parity": 0.85,
            "calibration": 0.9,
            "individual_fairness": 0.85,
            "counterfactual_fairness": 0.9,
            "treatment_equality": 0.8,
            "balance_for_positive": 0.85,
            "balance_for_negative": 0.85
        }
        return thresholds.get(self.value, 0.8)

    @property
    def group_metric(self) -> bool:
        """Whether this is a group-level metric."""
        group_metrics = {
            "demographic_parity", "equalized_odds", "equal_opportunity",
            "predictive_parity", "calibration", "treatment_equality",
            "balance_for_positive", "balance_for_negative"
        }
        return self.value in group_metrics


class TransparencyLevel(Enum):
    """Levels of AI transparency."""
    BLACK_BOX = "black_box"
    PARTIAL = "partial"
    EXPLAINABLE = "explainable"
    INTERPRETABLE = "interpretable"
    FULLY_TRANSPARENT = "fully_transparent"

    @property
    def score(self) -> int:
        scores = {
            "black_box": 0,
            "partial": 25,
            "explainable": 50,
            "interpretable": 75,
            "fully_transparent": 100
        }
        return scores.get(self.value, 0)

    @property
    def requirements(self) -> list:
        reqs = {
            "black_box": [],
            "partial": ["basic_documentation"],
            "explainable": ["feature_importance", "decision_factors", "basic_documentation"],
            "interpretable": ["feature_importance", "decision_factors", "model_rules", "counterfactual_explanations"],
            "fully_transparent": ["feature_importance", "decision_factors", "model_rules",
                                  "counterfactual_explanations", "full_audit_trail", "source_code"]
        }
        return reqs.get(self.value, [])


class StakeholderType(Enum):
    """Types of stakeholders affected by AI systems."""
    END_USER = "end_user"
    DATA_SUBJECT = "data_subject"
    OPERATOR = "operator"
    DEVELOPER = "developer"
    REGULATOR = "regulator"
    SOCIETY = "society"
    ENVIRONMENT = "environment"
    VULNERABLE_GROUP = "vulnerable_group"
    COMPETITOR = "competitor"
    EMPLOYEE = "employee"

    @property
    def impact_weight(self) -> float:
        """Weight for impact calculations (higher = more consideration)."""
        weights = {
            "end_user": 1.0,
            "data_subject": 1.0,
            "operator": 0.8,
            "developer": 0.6,
            "regulator": 0.7,
            "society": 0.9,
            "environment": 0.8,
            "vulnerable_group": 1.2,
            "competitor": 0.4,
            "employee": 0.8
        }
        return weights.get(self.value, 0.5)


class HarmCategory(Enum):
    """Categories of potential AI harms."""
    PHYSICAL = "physical"
    PSYCHOLOGICAL = "psychological"
    FINANCIAL = "financial"
    REPUTATIONAL = "reputational"
    PRIVACY = "privacy"
    AUTONOMY = "autonomy"
    DIGNITY = "dignity"
    DISCRIMINATION = "discrimination"
    MANIPULATION = "manipulation"
    SOCIETAL = "societal"
    ENVIRONMENTAL = "environmental"
    SECURITY = "security"

    @property
    def severity_multiplier(self) -> float:
        multipliers = {
            "physical": 2.0,
            "psychological": 1.5,
            "financial": 1.2,
            "reputational": 1.0,
            "privacy": 1.3,
            "autonomy": 1.4,
            "dignity": 1.5,
            "discrimination": 1.8,
            "manipulation": 1.6,
            "societal": 1.7,
            "environmental": 1.5,
            "security": 1.9
        }
        return multipliers.get(self.value, 1.0)


class EthicalPrinciple(Enum):
    """Core ethical principles for AI."""
    BENEFICENCE = "beneficence"
    NON_MALEFICENCE = "non_maleficence"
    AUTONOMY = "autonomy"
    JUSTICE = "justice"
    EXPLICABILITY = "explicability"
    PRIVACY = "privacy"
    ACCOUNTABILITY = "accountability"
    SUSTAINABILITY = "sustainability"
    HUMAN_OVERSIGHT = "human_oversight"
    ROBUSTNESS = "robustness"

    @property
    def description(self) -> str:
        descriptions = {
            "beneficence": "AI should benefit individuals and society",
            "non_maleficence": "AI should not cause harm",
            "autonomy": "AI should respect human agency and decision-making",
            "justice": "AI should be fair and non-discriminatory",
            "explicability": "AI decisions should be explainable",
            "privacy": "AI should protect personal data and privacy",
            "accountability": "Clear responsibility for AI outcomes",
            "sustainability": "AI should be environmentally sustainable",
            "human_oversight": "Humans should maintain control over AI",
            "robustness": "AI should be reliable and secure"
        }
        return descriptions.get(self.value, "")


class AssessmentStatus(Enum):
    """Status of ethics assessment."""
    NOT_STARTED = "not_started"
    IN_PROGRESS = "in_progress"
    UNDER_REVIEW = "under_review"
    APPROVED = "approved"
    APPROVED_WITH_CONDITIONS = "approved_with_conditions"
    REJECTED = "rejected"
    REQUIRES_REVISION = "requires_revision"
    EXPIRED = "expired"

    @property
    def is_final(self) -> bool:
        return self.value in {"approved", "rejected"}

    @property
    def allows_deployment(self) -> bool:
        return self.value in {"approved", "approved_with_conditions"}


class OversightLevel(Enum):
    """Levels of human oversight required."""
    NONE = "none"
    MONITORING = "monitoring"
    APPROVAL = "approval"
    HUMAN_IN_LOOP = "human_in_loop"
    HUMAN_ON_LOOP = "human_on_loop"
    FULL_MANUAL = "full_manual"

    @property
    def automation_allowed(self) -> float:
        """Percentage of automation allowed (0-100)."""
        levels = {
            "none": 100.0,
            "monitoring": 95.0,
            "approval": 80.0,
            "human_in_loop": 50.0,
            "human_on_loop": 70.0,
            "full_manual": 0.0
        }
        return levels.get(self.value, 50.0)


class MitigationStatus(Enum):
    """Status of bias/harm mitigation."""
    IDENTIFIED = "identified"
    ANALYZING = "analyzing"
    PLANNED = "planned"
    IN_PROGRESS = "in_progress"
    IMPLEMENTED = "implemented"
    VERIFIED = "verified"
    INEFFECTIVE = "ineffective"
    MONITORING = "monitoring"

    @property
    def is_active(self) -> bool:
        return self.value in {"analyzing", "planned", "in_progress"}


class AuditEventType(Enum):
    """Types of ethics audit events."""
    ASSESSMENT_STARTED = "assessment_started"
    BIAS_DETECTED = "bias_detected"
    MITIGATION_APPLIED = "mitigation_applied"
    STAKEHOLDER_CONSULTED = "stakeholder_consulted"
    PRINCIPLE_VIOLATION = "principle_violation"
    HARM_REPORTED = "harm_reported"
    OVERSIGHT_TRIGGERED = "oversight_triggered"
    APPEAL_FILED = "appeal_filed"
    DECISION_OVERRIDDEN = "decision_overridden"
    COMPLIANCE_CHECK = "compliance_check"
    EXTERNAL_AUDIT = "external_audit"

    @property
    def severity(self) -> str:
        severity_map = {
            "assessment_started": "info",
            "bias_detected": "warning",
            "mitigation_applied": "info",
            "stakeholder_consulted": "info",
            "principle_violation": "critical",
            "harm_reported": "critical",
            "oversight_triggered": "warning",
            "appeal_filed": "warning",
            "decision_overridden": "warning",
            "compliance_check": "info",
            "external_audit": "info"
        }
        return severity_map.get(self.value, "info")


# ============================================================
# DATA CLASSES
# ============================================================

@dataclass
class BiasInstance:
    """Detected bias instance in an AI system."""
    bias_id: str
    system_id: str
    category: BiasCategory
    description: str
    affected_groups: list = field(default_factory=list)
    severity: float = 0.5
    evidence: list = field(default_factory=list)
    detected_at: datetime = field(default_factory=datetime.now)
    mitigation_status: MitigationStatus = MitigationStatus.IDENTIFIED
    mitigation_plan: str = ""
    verified_by: str = ""

    def get_risk_score(self) -> float:
        """Calculate risk score based on severity and affected groups."""
        base_score = self.severity * self.category.mitigation_complexity
        group_multiplier = 1 + (len(self.affected_groups) * 0.1)
        return min(base_score * group_multiplier, 10.0)

    def is_mitigated(self) -> bool:
        return self.mitigation_status in {
            MitigationStatus.IMPLEMENTED,
            MitigationStatus.VERIFIED,
            MitigationStatus.MONITORING
        }

    def days_since_detection(self) -> int:
        return (datetime.now() - self.detected_at).days

    def needs_escalation(self) -> bool:
        """Check if bias needs escalation based on severity and time."""
        if self.severity >= 0.8 and not self.is_mitigated():
            return True
        if self.days_since_detection() > 30 and not self.is_mitigated():
            return True
        return False


@dataclass
class FairnessScore:
    """Fairness measurement for a specific metric."""
    score_id: str
    system_id: str
    metric: FairnessMetric
    value: float
    group_a: str
    group_b: str
    sample_size: int
    confidence_interval: tuple = (0.0, 0.0)
    measured_at: datetime = field(default_factory=datetime.now)
    dataset_version: str = ""

    def is_fair(self) -> bool:
        """Check if score meets fairness threshold."""
        return self.value >= self.metric.threshold

    def get_disparity(self) -> float:
        """Calculate disparity from perfect parity (1.0)."""
        return abs(1.0 - self.value)

    def get_grade(self) -> str:
        """Get letter grade for fairness score."""
        if self.value >= 0.95:
            return "A"
        elif self.value >= 0.85:
            return "B"
        elif self.value >= 0.75:
            return "C"
        elif self.value >= 0.65:
            return "D"
        return "F"

    def is_statistically_significant(self) -> bool:
        """Check if result is statistically significant."""
        if self.sample_size < 30:
            return False
        ci_width = self.confidence_interval[1] - self.confidence_interval[0]
        return ci_width < 0.2


@dataclass
class Stakeholder:
    """Stakeholder affected by an AI system."""
    stakeholder_id: str
    name: str
    stakeholder_type: StakeholderType
    population_size: int = 0
    vulnerability_factors: list = field(default_factory=list)
    interests: list = field(default_factory=list)
    potential_harms: list = field(default_factory=list)
    potential_benefits: list = field(default_factory=list)
    consent_obtained: bool = False
    engagement_level: str = "none"

    def get_impact_weight(self) -> float:
        """Calculate weighted impact considering vulnerability."""
        base_weight = self.stakeholder_type.impact_weight
        vulnerability_bonus = len(self.vulnerability_factors) * 0.1
        return min(base_weight + vulnerability_bonus, 2.0)

    def get_net_impact(self) -> float:
        """Calculate net impact (benefits - harms)."""
        benefit_score = len(self.potential_benefits)
        harm_score = len(self.potential_harms)
        return benefit_score - harm_score

    def needs_special_consideration(self) -> bool:
        return (
            len(self.vulnerability_factors) > 0 or
            self.stakeholder_type == StakeholderType.VULNERABLE_GROUP
        )


@dataclass
class HarmAssessment:
    """Assessment of potential harm from AI system."""
    harm_id: str
    system_id: str
    category: HarmCategory
    description: str
    likelihood: float = 0.5
    severity: float = 0.5
    affected_stakeholders: list = field(default_factory=list)
    reversibility: str = "partial"
    time_horizon: str = "immediate"
    mitigations: list = field(default_factory=list)
    residual_risk: float = 0.0
    assessed_by: str = ""
    assessed_at: datetime = field(default_factory=datetime.now)

    def get_risk_score(self) -> float:
        """Calculate risk score (likelihood * severity * multiplier)."""
        return (
            self.likelihood *
            self.severity *
            self.category.severity_multiplier
        )

    def get_risk_level(self) -> str:
        score = self.get_risk_score()
        if score >= 1.5:
            return "critical"
        elif score >= 1.0:
            return "high"
        elif score >= 0.5:
            return "medium"
        return "low"

    def is_acceptable(self, threshold: float = 0.5) -> bool:
        """Check if residual risk is acceptable."""
        return self.residual_risk <= threshold

    def get_mitigation_effectiveness(self) -> float:
        """Calculate how effective mitigations are."""
        if not self.mitigations:
            return 0.0
        initial_risk = self.get_risk_score()
        if initial_risk == 0:
            return 1.0
        return 1 - (self.residual_risk / initial_risk)


@dataclass
class EthicsAssessment:
    """Complete ethics assessment for an AI system."""
    assessment_id: str
    system_id: str
    system_name: str
    status: AssessmentStatus = AssessmentStatus.NOT_STARTED
    principles_evaluated: dict = field(default_factory=dict)
    bias_instances: list = field(default_factory=list)
    fairness_scores: list = field(default_factory=list)
    harm_assessments: list = field(default_factory=list)
    stakeholders: list = field(default_factory=list)
    transparency_level: TransparencyLevel = TransparencyLevel.BLACK_BOX
    oversight_level: OversightLevel = OversightLevel.NONE
    conditions: list = field(default_factory=list)
    assessor: str = ""
    reviewer: str = ""
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    expires_at: datetime = None

    def __post_init__(self):
        if self.expires_at is None:
            self.expires_at = self.created_at + timedelta(days=365)

    def get_overall_score(self) -> float:
        """Calculate overall ethics score (0-100)."""
        scores = []

        # Principles score
        if self.principles_evaluated:
            principle_scores = [
                v.get("score", 0) for v in self.principles_evaluated.values()
            ]
            scores.append(statistics.mean(principle_scores) if principle_scores else 0)

        # Fairness score
        if self.fairness_scores:
            fairness_values = [fs.value * 100 for fs in self.fairness_scores]
            scores.append(statistics.mean(fairness_values))

        # Bias score (inverse - fewer biases = higher score)
        unmitigated_biases = sum(1 for b in self.bias_instances if not b.is_mitigated())
        bias_penalty = min(unmitigated_biases * 10, 50)
        scores.append(100 - bias_penalty)

        # Harm score (inverse)
        high_risk_harms = sum(1 for h in self.harm_assessments if h.get_risk_level() in {"critical", "high"})
        harm_penalty = min(high_risk_harms * 15, 60)
        scores.append(100 - harm_penalty)

        # Transparency score
        scores.append(self.transparency_level.score)

        return statistics.mean(scores) if scores else 0

    def is_expired(self) -> bool:
        return datetime.now() > self.expires_at

    def get_compliance_gaps(self) -> list:
        """Identify gaps in ethical compliance."""
        gaps = []

        # Check unmitigated biases
        for bias in self.bias_instances:
            if not bias.is_mitigated():
                gaps.append(f"Unmitigated bias: {bias.category.value}")

        # Check failing fairness metrics
        for score in self.fairness_scores:
            if not score.is_fair():
                gaps.append(f"Fairness gap: {score.metric.value} ({score.value:.2f})")

        # Check high-risk harms
        for harm in self.harm_assessments:
            if harm.get_risk_level() in {"critical", "high"}:
                gaps.append(f"High risk harm: {harm.category.value}")

        # Check transparency requirements
        if self.transparency_level.score < 50:
            gaps.append("Insufficient transparency level")

        return gaps

    def can_approve(self) -> bool:
        """Check if assessment can be approved."""
        # No critical unmitigated issues
        critical_biases = any(
            b.severity >= 0.8 and not b.is_mitigated()
            for b in self.bias_instances
        )
        critical_harms = any(
            h.get_risk_level() == "critical" and not h.is_acceptable()
            for h in self.harm_assessments
        )
        return not critical_biases and not critical_harms


@dataclass
class OversightCheckpoint:
    """Human oversight checkpoint."""
    checkpoint_id: str
    system_id: str
    decision_type: str
    decision_data: dict = field(default_factory=dict)
    required_level: OversightLevel = OversightLevel.APPROVAL
    reviewer: str = ""
    decision: str = ""
    rationale: str = ""
    created_at: datetime = field(default_factory=datetime.now)
    decided_at: datetime = None
    deadline: datetime = None
    escalated: bool = False

    def is_pending(self) -> bool:
        return self.decided_at is None

    def is_overdue(self) -> bool:
        if self.deadline and self.is_pending():
            return datetime.now() > self.deadline
        return False

    def get_wait_time(self) -> timedelta:
        if self.decided_at:
            return self.decided_at - self.created_at
        return datetime.now() - self.created_at


@dataclass
class AppealRecord:
    """Record of an appeal against AI decision."""
    appeal_id: str
    system_id: str
    decision_id: str
    appellant: str
    grounds: str
    evidence: list = field(default_factory=list)
    status: str = "filed"
    reviewer: str = ""
    outcome: str = ""
    resolution: str = ""
    filed_at: datetime = field(default_factory=datetime.now)
    resolved_at: datetime = None

    def is_open(self) -> bool:
        return self.status in {"filed", "under_review"}

    def get_resolution_time(self) -> timedelta:
        if self.resolved_at:
            return self.resolved_at - self.filed_at
        return datetime.now() - self.filed_at


@dataclass
class EthicsAuditEntry:
    """Audit trail entry for ethics events."""
    entry_id: str
    system_id: str
    event_type: AuditEventType
    description: str
    actor: str
    data: dict = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    previous_hash: str = ""
    entry_hash: str = ""

    def __post_init__(self):
        if not self.entry_hash:
            self.entry_hash = self._calculate_hash()

    def _calculate_hash(self) -> str:
        content = f"{self.entry_id}{self.system_id}{self.event_type.value}"
        content += f"{self.description}{self.actor}{self.timestamp.isoformat()}"
        content += f"{self.previous_hash}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def verify_integrity(self) -> bool:
        return self.entry_hash == self._calculate_hash()


@dataclass
class AISystem:
    """AI system under ethics assessment."""
    system_id: str
    name: str
    description: str
    purpose: str
    domain: str
    deployment_context: str
    data_sources: list = field(default_factory=list)
    model_type: str = ""
    training_data_description: str = ""
    stakeholders: list = field(default_factory=list)
    assessments: list = field(default_factory=list)
    current_status: str = "development"
    risk_tier: str = "medium"
    created_at: datetime = field(default_factory=datetime.now)

    def get_latest_assessment(self) -> Optional[EthicsAssessment]:
        if not self.assessments:
            return None
        return max(self.assessments, key=lambda a: a.created_at)

    def needs_assessment(self) -> bool:
        latest = self.get_latest_assessment()
        if not latest:
            return True
        return latest.is_expired() or latest.status == AssessmentStatus.EXPIRED


# ============================================================
# ENGINE CLASSES
# ============================================================

class BiasAnalyzerEngine:
    """Engine for detecting and analyzing algorithmic bias."""

    BIAS_PATTERNS = {
        BiasCategory.DEMOGRAPHIC: {
            "indicators": ["disparate_impact", "unequal_representation", "proxy_discrimination"],
            "detection_confidence": 0.85
        },
        BiasCategory.SELECTION: {
            "indicators": ["sampling_bias", "exclusion_patterns", "convenience_sampling"],
            "detection_confidence": 0.80
        },
        BiasCategory.MEASUREMENT: {
            "indicators": ["proxy_variables", "inconsistent_measurement", "label_bias"],
            "detection_confidence": 0.75
        },
        BiasCategory.HISTORICAL: {
            "indicators": ["past_discrimination", "legacy_patterns", "inherited_bias"],
            "detection_confidence": 0.70
        }
    }

    def __init__(self):
        self.detected_biases: list = []
        self.analysis_history: list = []

    def analyze_for_bias(
        self,
        system_id: str,
        data_summary: dict,
        model_outputs: dict,
        protected_attributes: list
    ) -> list:
        """Analyze system for potential biases."""
        detected = []

        for category, config in self.BIAS_PATTERNS.items():
            bias_score = self._calculate_bias_score(
                category, data_summary, model_outputs, protected_attributes
            )

            if bias_score > 0.3:
                bias = BiasInstance(
                    bias_id=f"BIAS-{system_id[:4]}-{category.value[:4].upper()}-{len(detected)+1:03d}",
                    system_id=system_id,
                    category=category,
                    description=f"Potential {category.value} bias detected",
                    affected_groups=protected_attributes,
                    severity=bias_score,
                    evidence=[f"Detection method: {category.detection_method}"]
                )
                detected.append(bias)

        self.detected_biases.extend(detected)
        return detected

    def _calculate_bias_score(
        self,
        category: BiasCategory,
        data_summary: dict,
        model_outputs: dict,
        protected_attributes: list
    ) -> float:
        """Calculate bias score for a category."""
        base_score = 0.0

        # Check for demographic imbalance
        if category == BiasCategory.DEMOGRAPHIC:
            group_sizes = data_summary.get("group_distribution", {})
            if group_sizes:
                values = list(group_sizes.values())
                if values:
                    imbalance = max(values) / max(min(values), 1)
                    base_score = min((imbalance - 1) / 10, 1.0)

        # Check for outcome disparities
        if "outcome_rates" in model_outputs:
            rates = list(model_outputs["outcome_rates"].values())
            if len(rates) >= 2:
                disparity = max(rates) / max(min(rates), 0.01)
                base_score = max(base_score, min((disparity - 1) / 5, 1.0))

        return base_score

    def calculate_fairness_metrics(
        self,
        system_id: str,
        predictions: list,
        actuals: list,
        group_labels: list
    ) -> list:
        """Calculate fairness metrics across groups."""
        scores = []

        # Group predictions
        groups = {}
        for pred, actual, group in zip(predictions, actuals, group_labels):
            if group not in groups:
                groups[group] = {"predictions": [], "actuals": []}
            groups[group]["predictions"].append(pred)
            groups[group]["actuals"].append(actual)

        group_names = list(groups.keys())
        if len(group_names) < 2:
            return scores

        # Calculate demographic parity
        positive_rates = {}
        for group, data in groups.items():
            positive_rates[group] = sum(data["predictions"]) / max(len(data["predictions"]), 1)

        for i, group_a in enumerate(group_names):
            for group_b in group_names[i+1:]:
                ratio = min(positive_rates[group_a], positive_rates[group_b]) / max(
                    max(positive_rates[group_a], positive_rates[group_b]), 0.01
                )

                score = FairnessScore(
                    score_id=f"FAIR-{system_id[:4]}-{len(scores)+1:03d}",
                    system_id=system_id,
                    metric=FairnessMetric.DEMOGRAPHIC_PARITY,
                    value=ratio,
                    group_a=group_a,
                    group_b=group_b,
                    sample_size=len(predictions)
                )
                scores.append(score)

        return scores

    def suggest_mitigations(self, bias: BiasInstance) -> list:
        """Suggest mitigations for a detected bias."""
        mitigations = {
            BiasCategory.DEMOGRAPHIC: [
                "Rebalance training data across demographic groups",
                "Apply demographic parity constraints during training",
                "Use adversarial debiasing techniques",
                "Implement post-processing calibration"
            ],
            BiasCategory.SELECTION: [
                "Review and expand data collection criteria",
                "Use stratified sampling techniques",
                "Include underrepresented populations",
                "Apply selection bias correction methods"
            ],
            BiasCategory.MEASUREMENT: [
                "Audit proxy variables for discriminatory patterns",
                "Standardize measurement across groups",
                "Review and revise label definitions",
                "Use multiple independent measurements"
            ],
            BiasCategory.HISTORICAL: [
                "Remove historically biased features",
                "Use causal inference to identify discrimination",
                "Apply fairness-aware reweighting",
                "Implement counterfactual fairness constraints"
            ]
        }
        return mitigations.get(bias.category, ["Conduct manual review"])


class TransparencyEngine:
    """Engine for managing AI transparency and explainability."""

    TRANSPARENCY_REQUIREMENTS = {
        "high_stakes": {
            "minimum_level": TransparencyLevel.INTERPRETABLE,
            "required_explanations": ["feature_importance", "decision_factors", "counterfactual"],
            "documentation": ["model_card", "data_card", "impact_assessment"]
        },
        "medium_stakes": {
            "minimum_level": TransparencyLevel.EXPLAINABLE,
            "required_explanations": ["feature_importance", "decision_factors"],
            "documentation": ["model_card"]
        },
        "low_stakes": {
            "minimum_level": TransparencyLevel.PARTIAL,
            "required_explanations": ["basic_reasoning"],
            "documentation": []
        }
    }

    def __init__(self):
        self.explanations: dict = {}
        self.documentation: dict = {}

    def assess_transparency(self, system: AISystem) -> dict:
        """Assess transparency level of a system."""
        assessment = {
            "current_level": TransparencyLevel.BLACK_BOX,
            "required_level": TransparencyLevel.PARTIAL,
            "gaps": [],
            "recommendations": []
        }

        # Determine required level based on risk
        risk_mapping = {
            "critical": "high_stakes",
            "high": "high_stakes",
            "medium": "medium_stakes",
            "low": "low_stakes"
        }
        stakes = risk_mapping.get(system.risk_tier, "medium_stakes")
        requirements = self.TRANSPARENCY_REQUIREMENTS[stakes]
        assessment["required_level"] = requirements["minimum_level"]

        # Check what's available
        available_features = []
        if system.model_type:
            available_features.append("basic_documentation")
        if "interpretable" in system.model_type.lower():
            available_features.extend(["feature_importance", "model_rules"])

        # Determine current level
        for level in reversed(TransparencyLevel):
            if all(req in available_features for req in level.requirements):
                assessment["current_level"] = level
                break

        # Identify gaps
        current_reqs = set(assessment["current_level"].requirements)
        required_reqs = set(assessment["required_level"].requirements)
        assessment["gaps"] = list(required_reqs - current_reqs)

        return assessment

    def generate_explanation(
        self,
        system_id: str,
        decision_id: str,
        decision_data: dict,
        explanation_type: str = "feature_importance"
    ) -> dict:
        """Generate explanation for a decision."""
        explanation = {
            "explanation_id": f"EXP-{system_id[:4]}-{decision_id[:4]}",
            "type": explanation_type,
            "timestamp": datetime.now().isoformat(),
            "content": {}
        }

        if explanation_type == "feature_importance":
            # Mock feature importance calculation
            features = decision_data.get("features", {})
            importances = {
                k: abs(hash(k)) % 100 / 100 for k in features.keys()
            }
            total = sum(importances.values()) or 1
            explanation["content"] = {
                k: v/total for k, v in sorted(
                    importances.items(), key=lambda x: x[1], reverse=True
                )
            }

        elif explanation_type == "decision_factors":
            explanation["content"] = {
                "primary_factors": list(decision_data.get("features", {}).keys())[:3],
                "supporting_factors": list(decision_data.get("features", {}).keys())[3:6],
                "decision_rule": "threshold_based"
            }

        elif explanation_type == "counterfactual":
            explanation["content"] = {
                "original_outcome": decision_data.get("outcome", "unknown"),
                "counterfactual_changes": [
                    {"feature": k, "change_needed": "increase by 10%"}
                    for k in list(decision_data.get("features", {}).keys())[:2]
                ]
            }

        self.explanations[explanation["explanation_id"]] = explanation
        return explanation

    def create_model_card(self, system: AISystem) -> dict:
        """Create a model card for documentation."""
        return {
            "model_details": {
                "name": system.name,
                "version": "1.0",
                "type": system.model_type,
                "description": system.description
            },
            "intended_use": {
                "primary_use": system.purpose,
                "users": [s.name for s in system.stakeholders if hasattr(s, 'name')],
                "out_of_scope": []
            },
            "training_data": {
                "description": system.training_data_description,
                "sources": system.data_sources
            },
            "ethical_considerations": {
                "sensitive_data": True,
                "human_oversight": True,
                "potential_harms": []
            },
            "created_at": datetime.now().isoformat()
        }


class OversightArchitectEngine:
    """Engine for designing human oversight mechanisms."""

    OVERSIGHT_POLICIES = {
        "critical": {
            "level": OversightLevel.HUMAN_IN_LOOP,
            "review_frequency": "every_decision",
            "escalation_threshold": 0.3,
            "appeal_window_days": 30
        },
        "high": {
            "level": OversightLevel.HUMAN_ON_LOOP,
            "review_frequency": "sampled",
            "sample_rate": 0.1,
            "escalation_threshold": 0.5,
            "appeal_window_days": 14
        },
        "medium": {
            "level": OversightLevel.APPROVAL,
            "review_frequency": "batch",
            "batch_size": 100,
            "escalation_threshold": 0.7,
            "appeal_window_days": 7
        },
        "low": {
            "level": OversightLevel.MONITORING,
            "review_frequency": "periodic",
            "period_days": 7,
            "escalation_threshold": 0.9,
            "appeal_window_days": 7
        }
    }

    def __init__(self):
        self.checkpoints: list = []
        self.appeals: list = []

    def design_oversight(self, system: AISystem) -> dict:
        """Design oversight mechanisms for a system."""
        policy = self.OVERSIGHT_POLICIES.get(system.risk_tier, self.OVERSIGHT_POLICIES["medium"])

        return {
            "system_id": system.system_id,
            "oversight_level": policy["level"],
            "review_process": {
                "frequency": policy["review_frequency"],
                "escalation_threshold": policy["escalation_threshold"]
            },
            "appeal_process": {
                "enabled": True,
                "window_days": policy["appeal_window_days"],
                "reviewers": ["ethics_committee"]
            },
            "override_capability": {
                "enabled": policy["level"] != OversightLevel.NONE,
                "authorized_roles": ["supervisor", "ethics_officer"]
            }
        }

    def create_checkpoint(
        self,
        system_id: str,
        decision_type: str,
        decision_data: dict,
        risk_level: str = "medium"
    ) -> OversightCheckpoint:
        """Create an oversight checkpoint for human review."""
        policy = self.OVERSIGHT_POLICIES.get(risk_level, self.OVERSIGHT_POLICIES["medium"])

        checkpoint = OversightCheckpoint(
            checkpoint_id=f"CHK-{system_id[:4]}-{len(self.checkpoints)+1:04d}",
            system_id=system_id,
            decision_type=decision_type,
            decision_data=decision_data,
            required_level=policy["level"],
            deadline=datetime.now() + timedelta(days=3)
        )

        self.checkpoints.append(checkpoint)
        return checkpoint

    def file_appeal(
        self,
        system_id: str,
        decision_id: str,
        appellant: str,
        grounds: str
    ) -> AppealRecord:
        """File an appeal against an AI decision."""
        appeal = AppealRecord(
            appeal_id=f"APL-{system_id[:4]}-{len(self.appeals)+1:04d}",
            system_id=system_id,
            decision_id=decision_id,
            appellant=appellant,
            grounds=grounds
        )

        self.appeals.append(appeal)
        return appeal

    def process_appeal(self, appeal_id: str, reviewer: str, outcome: str, resolution: str):
        """Process an appeal."""
        for appeal in self.appeals:
            if appeal.appeal_id == appeal_id:
                appeal.reviewer = reviewer
                appeal.outcome = outcome
                appeal.resolution = resolution
                appeal.status = "resolved"
                appeal.resolved_at = datetime.now()
                break


class ImpactAssessorEngine:
    """Engine for assessing AI impact on stakeholders."""

    HARM_ASSESSMENT_MATRIX = {
        HarmCategory.PHYSICAL: {
            "questions": [
                "Can the system cause physical injury?",
                "Does it control physical actuators?",
                "Could errors lead to safety incidents?"
            ],
            "mitigations": ["safety_interlocks", "human_override", "fail_safe_defaults"]
        },
        HarmCategory.DISCRIMINATION: {
            "questions": [
                "Does the system make decisions about individuals?",
                "Are protected characteristics involved?",
                "Could outcomes differ by demographic group?"
            ],
            "mitigations": ["fairness_constraints", "bias_monitoring", "appeal_process"]
        },
        HarmCategory.PRIVACY: {
            "questions": [
                "Does the system process personal data?",
                "Is data used beyond original consent?",
                "Can individuals be re-identified?"
            ],
            "mitigations": ["data_minimization", "anonymization", "access_controls"]
        },
        HarmCategory.AUTONOMY: {
            "questions": [
                "Does the system limit user choices?",
                "Are users aware of AI influence?",
                "Can users opt out?"
            ],
            "mitigations": ["transparency", "user_control", "opt_out_option"]
        }
    }

    def __init__(self):
        self.assessments: list = []
        self.stakeholder_registry: list = []

    def identify_stakeholders(self, system: AISystem) -> list:
        """Identify stakeholders for a system."""
        stakeholders = []

        # Standard stakeholder types
        standard_stakeholders = [
            ("End Users", StakeholderType.END_USER, "Users interacting with the system"),
            ("Data Subjects", StakeholderType.DATA_SUBJECT, "Individuals whose data is processed"),
            ("Operators", StakeholderType.OPERATOR, "Staff operating the system"),
            ("Regulators", StakeholderType.REGULATOR, "Regulatory bodies with oversight")
        ]

        for name, stype, interest in standard_stakeholders:
            stakeholder = Stakeholder(
                stakeholder_id=f"STK-{system.system_id[:4]}-{len(stakeholders)+1:03d}",
                name=name,
                stakeholder_type=stype,
                interests=[interest]
            )
            stakeholders.append(stakeholder)

        # Add vulnerable groups if high-risk
        if system.risk_tier in {"critical", "high"}:
            vulnerable = Stakeholder(
                stakeholder_id=f"STK-{system.system_id[:4]}-{len(stakeholders)+1:03d}",
                name="Vulnerable Populations",
                stakeholder_type=StakeholderType.VULNERABLE_GROUP,
                vulnerability_factors=["limited_technical_literacy", "power_imbalance"],
                interests=["protection", "fair_treatment"]
            )
            stakeholders.append(vulnerable)

        return stakeholders

    def assess_harms(
        self,
        system: AISystem,
        stakeholders: list
    ) -> list:
        """Assess potential harms for stakeholders."""
        assessments = []

        for category, config in self.HARM_ASSESSMENT_MATRIX.items():
            # Simulate risk assessment
            likelihood = 0.2 + (0.1 * len([
                s for s in stakeholders
                if s.stakeholder_type == StakeholderType.VULNERABLE_GROUP
            ]))

            severity = category.severity_multiplier * 0.3

            if likelihood * severity > 0.1:
                assessment = HarmAssessment(
                    harm_id=f"HARM-{system.system_id[:4]}-{category.value[:4].upper()}-{len(assessments)+1:02d}",
                    system_id=system.system_id,
                    category=category,
                    description=f"Potential {category.value} harm",
                    likelihood=likelihood,
                    severity=severity,
                    affected_stakeholders=[s.stakeholder_id for s in stakeholders],
                    mitigations=config["mitigations"]
                )
                assessments.append(assessment)

        self.assessments.extend(assessments)
        return assessments

    def calculate_net_benefit(
        self,
        benefits: list,
        harms: list,
        stakeholders: list
    ) -> dict:
        """Calculate net benefit analysis."""
        total_benefit = 0.0
        total_harm = 0.0

        # Calculate weighted benefits
        for benefit in benefits:
            weight = 1.0
            for stakeholder in stakeholders:
                if stakeholder.stakeholder_id in benefit.get("beneficiaries", []):
                    weight *= stakeholder.get_impact_weight()
            total_benefit += benefit.get("magnitude", 0.5) * weight

        # Calculate weighted harms
        for harm in harms:
            harm_score = harm.get_risk_score()
            stakeholder_weight = sum(
                s.get_impact_weight()
                for s in stakeholders
                if s.stakeholder_id in harm.affected_stakeholders
            ) / max(len(harm.affected_stakeholders), 1)
            total_harm += harm_score * stakeholder_weight

        return {
            "total_benefit": total_benefit,
            "total_harm": total_harm,
            "net_benefit": total_benefit - total_harm,
            "benefit_harm_ratio": total_benefit / max(total_harm, 0.01),
            "recommendation": "proceed" if total_benefit > total_harm else "reconsider"
        }


class EthicsAssessmentEngine:
    """Main engine for comprehensive ethics assessments."""

    PRINCIPLE_CHECKLIST = {
        EthicalPrinciple.BENEFICENCE: [
            "System provides clear value to users",
            "Benefits outweigh risks",
            "Positive societal impact considered"
        ],
        EthicalPrinciple.NON_MALEFICENCE: [
            "Potential harms identified and mitigated",
            "No intentional harm designed",
            "Unintended consequences considered"
        ],
        EthicalPrinciple.AUTONOMY: [
            "User consent obtained appropriately",
            "Users can opt out",
            "Human agency preserved"
        ],
        EthicalPrinciple.JUSTICE: [
            "Fair treatment across groups",
            "No discriminatory outcomes",
            "Equitable access to benefits"
        ],
        EthicalPrinciple.EXPLICABILITY: [
            "Decisions can be explained",
            "Transparency requirements met",
            "Documentation available"
        ]
    }

    def __init__(self):
        self.bias_analyzer = BiasAnalyzerEngine()
        self.transparency_engine = TransparencyEngine()
        self.oversight_architect = OversightArchitectEngine()
        self.impact_assessor = ImpactAssessorEngine()
        self.assessments: dict = {}
        self.audit_trail: list = []

    def create_assessment(self, system: AISystem) -> EthicsAssessment:
        """Create a new ethics assessment."""
        assessment = EthicsAssessment(
            assessment_id=f"ASSESS-{system.system_id[:4]}-{datetime.now().strftime('%Y%m%d')}",
            system_id=system.system_id,
            system_name=system.name,
            status=AssessmentStatus.IN_PROGRESS
        )

        # Log creation
        self._log_event(
            system.system_id,
            AuditEventType.ASSESSMENT_STARTED,
            f"Ethics assessment created for {system.name}",
            "system"
        )

        self.assessments[assessment.assessment_id] = assessment
        return assessment

    def run_full_assessment(self, system: AISystem) -> EthicsAssessment:
        """Run complete ethics assessment."""
        assessment = self.create_assessment(system)

        # 1. Identify stakeholders
        stakeholders = self.impact_assessor.identify_stakeholders(system)
        assessment.stakeholders = stakeholders

        # 2. Assess potential harms
        harms = self.impact_assessor.assess_harms(system, stakeholders)
        assessment.harm_assessments = harms

        # 3. Analyze for bias
        biases = self.bias_analyzer.analyze_for_bias(
            system.system_id,
            {"group_distribution": {}},
            {"outcome_rates": {}},
            ["age", "gender", "race"]
        )
        assessment.bias_instances = biases

        # 4. Assess transparency
        transparency = self.transparency_engine.assess_transparency(system)
        assessment.transparency_level = transparency["current_level"]

        # 5. Design oversight
        oversight = self.oversight_architect.design_oversight(system)
        assessment.oversight_level = oversight["oversight_level"]

        # 6. Evaluate principles
        for principle in EthicalPrinciple:
            checklist = self.PRINCIPLE_CHECKLIST.get(principle, [])
            # Simulate scoring
            score = 70 + (hash(principle.value) % 30)
            assessment.principles_evaluated[principle.value] = {
                "score": score,
                "checklist": checklist,
                "notes": []
            }

        # 7. Determine status
        if assessment.can_approve():
            if assessment.get_compliance_gaps():
                assessment.status = AssessmentStatus.APPROVED_WITH_CONDITIONS
                assessment.conditions = assessment.get_compliance_gaps()
            else:
                assessment.status = AssessmentStatus.APPROVED
        else:
            assessment.status = AssessmentStatus.REQUIRES_REVISION

        assessment.updated_at = datetime.now()
        return assessment

    def _log_event(
        self,
        system_id: str,
        event_type: AuditEventType,
        description: str,
        actor: str
    ):
        """Log an ethics audit event."""
        previous_hash = self.audit_trail[-1].entry_hash if self.audit_trail else ""

        entry = EthicsAuditEntry(
            entry_id=f"AUD-{len(self.audit_trail)+1:06d}",
            system_id=system_id,
            event_type=event_type,
            description=description,
            actor=actor,
            previous_hash=previous_hash
        )

        self.audit_trail.append(entry)

    def get_assessment_report(self, assessment_id: str) -> dict:
        """Generate assessment report."""
        assessment = self.assessments.get(assessment_id)
        if not assessment:
            return {"error": "Assessment not found"}

        return {
            "assessment_id": assessment.assessment_id,
            "system": assessment.system_name,
            "status": assessment.status.value,
            "overall_score": assessment.get_overall_score(),
            "transparency_level": assessment.transparency_level.value,
            "oversight_level": assessment.oversight_level.value,
            "bias_count": len(assessment.bias_instances),
            "unmitigated_biases": sum(1 for b in assessment.bias_instances if not b.is_mitigated()),
            "harm_count": len(assessment.harm_assessments),
            "high_risk_harms": sum(1 for h in assessment.harm_assessments if h.get_risk_level() in {"critical", "high"}),
            "compliance_gaps": assessment.get_compliance_gaps(),
            "principles": {
                p: v.get("score", 0) for p, v in assessment.principles_evaluated.items()
            },
            "recommendations": self._generate_recommendations(assessment),
            "generated_at": datetime.now().isoformat()
        }

    def _generate_recommendations(self, assessment: EthicsAssessment) -> list:
        """Generate recommendations based on assessment."""
        recommendations = []

        # Bias recommendations
        for bias in assessment.bias_instances:
            if not bias.is_mitigated():
                recommendations.append({
                    "priority": "high" if bias.severity >= 0.7 else "medium",
                    "category": "bias",
                    "action": f"Mitigate {bias.category.value} bias",
                    "details": self.bias_analyzer.suggest_mitigations(bias)
                })

        # Harm recommendations
        for harm in assessment.harm_assessments:
            if harm.get_risk_level() in {"critical", "high"}:
                recommendations.append({
                    "priority": "critical" if harm.get_risk_level() == "critical" else "high",
                    "category": "harm",
                    "action": f"Address {harm.category.value} risk",
                    "details": harm.mitigations
                })

        # Transparency recommendations
        if assessment.transparency_level.score < 50:
            recommendations.append({
                "priority": "medium",
                "category": "transparency",
                "action": "Improve transparency level",
                "details": assessment.transparency_level.requirements
            })

        return sorted(recommendations, key=lambda r: {"critical": 0, "high": 1, "medium": 2, "low": 3}[r["priority"]])


# ============================================================
# REPORTER CLASS
# ============================================================

class EthicsReporter:
    """Reporter for ethics assessment visualization."""

    @staticmethod
    def generate_assessment_report(assessment: EthicsAssessment) -> str:
        """Generate formatted assessment report."""
        lines = []
        lines.append("=" * 60)
        lines.append("           AI ETHICS ASSESSMENT REPORT")
        lines.append("=" * 60)
        lines.append("")
        lines.append(f"System: {assessment.system_name}")
        lines.append(f"Assessment ID: {assessment.assessment_id}")
        lines.append(f"Status: {EthicsReporter._status_icon(assessment.status)} {assessment.status.value.upper()}")
        lines.append(f"Date: {assessment.updated_at.strftime('%Y-%m-%d %H:%M')}")
        lines.append("")

        # Overall Score
        score = assessment.get_overall_score()
        lines.append("OVERALL ETHICS SCORE")
        lines.append("-" * 40)
        lines.append(f"  Score: {score:.1f}/100 {EthicsReporter._grade_icon(score)}")
        lines.append(f"  {EthicsReporter._progress_bar(score, 100)}")
        lines.append("")

        # Transparency & Oversight
        lines.append("TRANSPARENCY & OVERSIGHT")
        lines.append("-" * 40)
        lines.append(f"  Transparency: {assessment.transparency_level.value}")
        lines.append(f"    {EthicsReporter._progress_bar(assessment.transparency_level.score, 100)}")
        lines.append(f"  Oversight: {assessment.oversight_level.value}")
        lines.append(f"    Automation: {assessment.oversight_level.automation_allowed:.0f}%")
        lines.append("")

        # Bias Analysis
        lines.append("BIAS ANALYSIS")
        lines.append("-" * 40)
        total_biases = len(assessment.bias_instances)
        mitigated = sum(1 for b in assessment.bias_instances if b.is_mitigated())
        lines.append(f"  Total Detected: {total_biases}")
        lines.append(f"  Mitigated: {mitigated}/{total_biases}")
        for bias in assessment.bias_instances[:5]:
            status = "â—" if bias.is_mitigated() else "â—‹"
            lines.append(f"    {status} {bias.category.value}: {bias.severity:.2f} severity")
        lines.append("")

        # Harm Assessment
        lines.append("HARM ASSESSMENT")
        lines.append("-" * 40)
        for harm in assessment.harm_assessments[:5]:
            risk = harm.get_risk_level()
            icon = {"critical": "ðŸ”´", "high": "ðŸŸ ", "medium": "ðŸŸ¡", "low": "ðŸŸ¢"}[risk]
            lines.append(f"  {icon} {harm.category.value}: {risk.upper()}")
        lines.append("")

        # Principles
        if assessment.principles_evaluated:
            lines.append("ETHICAL PRINCIPLES")
            lines.append("-" * 40)
            for principle, data in assessment.principles_evaluated.items():
                score_val = data.get("score", 0)
                bar = EthicsReporter._mini_bar(score_val)
                lines.append(f"  {principle}: {bar} {score_val:.0f}")
        lines.append("")

        # Compliance Gaps
        gaps = assessment.get_compliance_gaps()
        if gaps:
            lines.append("COMPLIANCE GAPS")
            lines.append("-" * 40)
            for gap in gaps[:5]:
                lines.append(f"  âš  {gap}")
        lines.append("")

        # Conditions
        if assessment.conditions:
            lines.append("CONDITIONS FOR APPROVAL")
            lines.append("-" * 40)
            for condition in assessment.conditions:
                lines.append(f"  â€¢ {condition}")

        lines.append("")
        lines.append("=" * 60)
        return "\n".join(lines)

    @staticmethod
    def _status_icon(status: AssessmentStatus) -> str:
        icons = {
            AssessmentStatus.APPROVED: "âœ…",
            AssessmentStatus.APPROVED_WITH_CONDITIONS: "âš ï¸",
            AssessmentStatus.REJECTED: "âŒ",
            AssessmentStatus.REQUIRES_REVISION: "ðŸ”„",
            AssessmentStatus.IN_PROGRESS: "â³",
            AssessmentStatus.UNDER_REVIEW: "ðŸ‘ï¸",
            AssessmentStatus.EXPIRED: "â°"
        }
        return icons.get(status, "â—‹")

    @staticmethod
    def _grade_icon(score: float) -> str:
        if score >= 90:
            return "ðŸ†"
        elif score >= 80:
            return "âœ“"
        elif score >= 70:
            return "~"
        elif score >= 60:
            return "âš "
        return "âœ—"

    @staticmethod
    def _progress_bar(value: float, max_val: float, width: int = 20) -> str:
        filled = int((value / max_val) * width)
        return f"[{'â–ˆ' * filled}{'â–‘' * (width - filled)}]"

    @staticmethod
    def _mini_bar(value: float, width: int = 10) -> str:
        filled = int((value / 100) * width)
        return f"{'â–ˆ' * filled}{'â–‘' * (width - filled)}"


# ============================================================
# CLI INTERFACE
# ============================================================

def main():
    """Main CLI entry point."""
    import argparse

    parser = argparse.ArgumentParser(
        description="AI.ETHICS.EXE - AI Ethics & Responsible Use OS",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # assess command
    assess_parser = subparsers.add_parser("assess", help="Run ethics assessment")
    assess_parser.add_argument("--system-id", required=True, help="System ID to assess")
    assess_parser.add_argument("--name", required=True, help="System name")
    assess_parser.add_argument("--purpose", required=True, help="System purpose")
    assess_parser.add_argument("--risk-tier", choices=["low", "medium", "high", "critical"], default="medium")

    # bias command
    bias_parser = subparsers.add_parser("bias", help="Analyze for bias")
    bias_parser.add_argument("--system-id", required=True, help="System ID")
    bias_parser.add_argument("--protected-attrs", nargs="+", default=["age", "gender", "race"])

    # transparency command
    trans_parser = subparsers.add_parser("transparency", help="Assess transparency")
    trans_parser.add_argument("--system-id", required=True, help="System ID")
    trans_parser.add_argument("--model-type", default="neural_network")

    # oversight command
    oversight_parser = subparsers.add_parser("oversight", help="Design oversight")
    oversight_parser.add_argument("--system-id", required=True, help="System ID")
    oversight_parser.add_argument("--risk-tier", choices=["low", "medium", "high", "critical"], default="medium")

    # report command
    report_parser = subparsers.add_parser("report", help="Generate assessment report")
    report_parser.add_argument("--assessment-id", required=True, help="Assessment ID")

    args = parser.parse_args()

    engine = EthicsAssessmentEngine()

    if args.command == "assess":
        system = AISystem(
            system_id=args.system_id,
            name=args.name,
            description=args.purpose,
            purpose=args.purpose,
            domain="general",
            deployment_context="production",
            risk_tier=args.risk_tier
        )
        assessment = engine.run_full_assessment(system)
        print(EthicsReporter.generate_assessment_report(assessment))

    elif args.command == "bias":
        biases = engine.bias_analyzer.analyze_for_bias(
            args.system_id,
            {"group_distribution": {"a": 100, "b": 50}},
            {"outcome_rates": {"a": 0.8, "b": 0.5}},
            args.protected_attrs
        )
        print(f"Detected {len(biases)} bias instances")
        for bias in biases:
            print(f"  - {bias.category.value}: severity {bias.severity:.2f}")

    elif args.command == "transparency":
        system = AISystem(
            system_id=args.system_id,
            name="Test System",
            description="Test",
            purpose="Test",
            domain="test",
            deployment_context="test",
            model_type=args.model_type
        )
        result = engine.transparency_engine.assess_transparency(system)
        print(f"Current Level: {result['current_level'].value}")
        print(f"Required Level: {result['required_level'].value}")
        print(f"Gaps: {result['gaps']}")

    elif args.command == "oversight":
        system = AISystem(
            system_id=args.system_id,
            name="Test System",
            description="Test",
            purpose="Test",
            domain="test",
            deployment_context="test",
            risk_tier=args.risk_tier
        )
        result = engine.oversight_architect.design_oversight(system)
        print(f"Oversight Level: {result['oversight_level'].value}")
        print(f"Review Process: {result['review_process']}")
        print(f"Appeal Process: {result['appeal_process']}")

    elif args.command == "report":
        report = engine.get_assessment_report(args.assessment_id)
        if "error" in report:
            print(f"Error: {report['error']}")
        else:
            for key, value in report.items():
                print(f"{key}: {value}")

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
```

---

## ETHICS DOMAINS

| Domain | Focus | Key Metric |
|--------|-------|------------|
| Fairness | Equal treatment | Demographic parity |
| Transparency | Explainable | Decision clarity |
| Accountability | Oversight | Review coverage |
| Privacy | Data protection | Consent rate |
| Safety | Harm prevention | Incident rate |

## OUTPUT FORMAT

```
ETHICS ASSESSMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
System: [system_name]
Assessment Type: [initial/review/audit]
Date: [timestamp]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RISK OVERVIEW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       ETHICAL RISK SUMMARY          â”‚
â”‚                                     â”‚
â”‚  Overall Risk: [â—/â—/â—‹] [level]      â”‚
â”‚  Compliance: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ [X]%        â”‚
â”‚                                     â”‚
â”‚  Bias Risk: [H/M/L]                 â”‚
â”‚  Transparency: [H/M/L]              â”‚
â”‚  Oversight: [H/M/L]                 â”‚
â”‚  Privacy: [H/M/L]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAKEHOLDER IMPACT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
| Stakeholder | Impact | Risk | Mitigation |
|-------------|--------|------|------------|
| [stakeholder_1] | [impact] | [H/M/L] | [action] |
| [stakeholder_2] | [impact] | [H/M/L] | [action] |
| [stakeholder_3] | [impact] | [H/M/L] | [action] |

BIAS ASSESSMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bias Vectors Identified:           â”‚
â”‚  â€¢ [bias_1]: [severity]             â”‚
â”‚  â€¢ [bias_2]: [severity]             â”‚
â”‚                                     â”‚
â”‚  Mitigation Status:                 â”‚
â”‚  â€¢ [mitigation_1]: [â—/â—‹]            â”‚
â”‚  â€¢ [mitigation_2]: [â—/â—‹]            â”‚
â”‚                                     â”‚
â”‚  Fairness Score: [X]/100            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRANSPARENCY REQUIREMENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
| Requirement | Status | Gap |
|-------------|--------|-----|
| Decision explanation | [â—/â—‹] | [gap] |
| Data provenance | [â—/â—‹] | [gap] |
| Model documentation | [â—/â—‹] | [gap] |
| User notification | [â—/â—‹] | [gap] |

OVERSIGHT MECHANISMS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Human Review Points:               â”‚
â”‚  â€¢ [review_point_1]                 â”‚
â”‚  â€¢ [review_point_2]                 â”‚
â”‚                                     â”‚
â”‚  Appeal Process: [defined/pending]  â”‚
â”‚  Override Protocol: [enabled/none]  â”‚
â”‚                                     â”‚
â”‚  Accountability: [owner]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SAFEGUARDS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
| Safeguard | Purpose | Status |
|-----------|---------|--------|
| [safeguard_1] | [purpose] | [â—/â—‹] |
| [safeguard_2] | [purpose] | [â—/â—‹] |
| [safeguard_3] | [purpose] | [â—/â—‹] |

RECOMMENDATIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
| Priority | Action | Impact |
|----------|--------|--------|
| 1 | [action_1] | [impact] |
| 2 | [action_2] | [impact] |
| 3 | [action_3] | [impact] |

Ethics Status: â— Assessment Complete
```

## QUICK COMMANDS

- `/ai-ethics assess [system]` - Full ethics assessment
- `/ai-ethics bias [model]` - Bias analysis
- `/ai-ethics transparency` - Transparency audit
- `/ai-ethics oversight` - Review oversight mechanisms
- `/ai-ethics report` - Generate compliance report

$ARGUMENTS
