# AWS S3 Storage

Production-ready AWS S3 integration with uploads, downloads, presigned URLs, and lifecycle management.

## Overview

Amazon S3 provides scalable object storage with industry-leading durability, availability, and performance.

## Quick Start

```bash
npm install @aws-sdk/client-s3 @aws-sdk/s3-request-presigner @aws-sdk/lib-storage
```

## TypeScript Implementation

### S3 Client Configuration

```typescript
// src/storage/s3-client.ts
import {
  S3Client,
  PutObjectCommand,
  GetObjectCommand,
  DeleteObjectCommand,
  ListObjectsV2Command,
  CopyObjectCommand,
  HeadObjectCommand,
  CreateMultipartUploadCommand,
  UploadPartCommand,
  CompleteMultipartUploadCommand,
  AbortMultipartUploadCommand,
} from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';
import { Upload } from '@aws-sdk/lib-storage';
import { Readable } from 'stream';

interface S3Config {
  region: string;
  bucket: string;
  accessKeyId?: string;
  secretAccessKey?: string;
  endpoint?: string;
}

interface UploadOptions {
  contentType?: string;
  metadata?: Record<string, string>;
  acl?: 'private' | 'public-read' | 'authenticated-read';
  cacheControl?: string;
  contentDisposition?: string;
  serverSideEncryption?: 'AES256' | 'aws:kms';
  tagging?: Record<string, string>;
}

interface PresignedUrlOptions {
  expiresIn?: number;
  responseContentType?: string;
  responseContentDisposition?: string;
}

interface ListOptions {
  prefix?: string;
  maxKeys?: number;
  continuationToken?: string;
  delimiter?: string;
}

interface S3Object {
  key: string;
  size: number;
  lastModified: Date;
  etag: string;
  storageClass?: string;
}

interface ListResult {
  objects: S3Object[];
  prefixes: string[];
  isTruncated: boolean;
  continuationToken?: string;
}

export class S3Storage {
  private client: S3Client;
  private bucket: string;

  constructor(config: S3Config) {
    this.bucket = config.bucket;

    this.client = new S3Client({
      region: config.region,
      ...(config.accessKeyId && config.secretAccessKey && {
        credentials: {
          accessKeyId: config.accessKeyId,
          secretAccessKey: config.secretAccessKey,
        },
      }),
      ...(config.endpoint && {
        endpoint: config.endpoint,
        forcePathStyle: true,
      }),
    });
  }

  // Upload file from buffer
  async upload(
    key: string,
    body: Buffer | Readable | string,
    options: UploadOptions = {}
  ): Promise<{ key: string; etag: string; location: string }> {
    const command = new PutObjectCommand({
      Bucket: this.bucket,
      Key: key,
      Body: body,
      ContentType: options.contentType,
      Metadata: options.metadata,
      ACL: options.acl,
      CacheControl: options.cacheControl,
      ContentDisposition: options.contentDisposition,
      ServerSideEncryption: options.serverSideEncryption,
      Tagging: options.tagging
        ? Object.entries(options.tagging)
            .map(([k, v]) => `${encodeURIComponent(k)}=${encodeURIComponent(v)}`)
            .join('&')
        : undefined,
    });

    const result = await this.client.send(command);

    return {
      key,
      etag: result.ETag?.replace(/"/g, '') || '',
      location: `https://${this.bucket}.s3.amazonaws.com/${key}`,
    };
  }

  // Upload large files with multipart
  async uploadLarge(
    key: string,
    body: Readable | Buffer,
    options: UploadOptions & { partSize?: number; queueSize?: number } = {}
  ): Promise<{ key: string; etag: string; location: string }> {
    const upload = new Upload({
      client: this.client,
      params: {
        Bucket: this.bucket,
        Key: key,
        Body: body,
        ContentType: options.contentType,
        Metadata: options.metadata,
        ACL: options.acl,
        CacheControl: options.cacheControl,
        ServerSideEncryption: options.serverSideEncryption,
      },
      partSize: options.partSize || 5 * 1024 * 1024, // 5MB default
      queueSize: options.queueSize || 4,
    });

    upload.on('httpUploadProgress', (progress) => {
      console.log(`Upload progress: ${progress.loaded}/${progress.total}`);
    });

    const result = await upload.done();

    return {
      key,
      etag: result.ETag?.replace(/"/g, '') || '',
      location: result.Location || `https://${this.bucket}.s3.amazonaws.com/${key}`,
    };
  }

  // Download file
  async download(key: string): Promise<{
    body: Readable;
    contentType?: string;
    contentLength?: number;
    metadata?: Record<string, string>;
  }> {
    const command = new GetObjectCommand({
      Bucket: this.bucket,
      Key: key,
    });

    const result = await this.client.send(command);

    return {
      body: result.Body as Readable,
      contentType: result.ContentType,
      contentLength: result.ContentLength,
      metadata: result.Metadata,
    };
  }

  // Download to buffer
  async downloadToBuffer(key: string): Promise<Buffer> {
    const { body } = await this.download(key);
    const chunks: Buffer[] = [];

    for await (const chunk of body) {
      chunks.push(Buffer.from(chunk));
    }

    return Buffer.concat(chunks);
  }

  // Delete file
  async delete(key: string): Promise<void> {
    const command = new DeleteObjectCommand({
      Bucket: this.bucket,
      Key: key,
    });

    await this.client.send(command);
  }

  // Delete multiple files
  async deleteMany(keys: string[]): Promise<{ deleted: string[]; errors: string[] }> {
    const deleted: string[] = [];
    const errors: string[] = [];

    // S3 DeleteObjects can handle up to 1000 keys at once
    const chunks = this.chunkArray(keys, 1000);

    for (const chunk of chunks) {
      const results = await Promise.allSettled(
        chunk.map((key) => this.delete(key))
      );

      results.forEach((result, index) => {
        if (result.status === 'fulfilled') {
          deleted.push(chunk[index]);
        } else {
          errors.push(chunk[index]);
        }
      });
    }

    return { deleted, errors };
  }

  // Check if file exists
  async exists(key: string): Promise<boolean> {
    try {
      await this.getMetadata(key);
      return true;
    } catch (error: any) {
      if (error.name === 'NotFound') {
        return false;
      }
      throw error;
    }
  }

  // Get file metadata
  async getMetadata(key: string): Promise<{
    contentType?: string;
    contentLength?: number;
    lastModified?: Date;
    etag?: string;
    metadata?: Record<string, string>;
  }> {
    const command = new HeadObjectCommand({
      Bucket: this.bucket,
      Key: key,
    });

    const result = await this.client.send(command);

    return {
      contentType: result.ContentType,
      contentLength: result.ContentLength,
      lastModified: result.LastModified,
      etag: result.ETag?.replace(/"/g, ''),
      metadata: result.Metadata,
    };
  }

  // List objects
  async list(options: ListOptions = {}): Promise<ListResult> {
    const command = new ListObjectsV2Command({
      Bucket: this.bucket,
      Prefix: options.prefix,
      MaxKeys: options.maxKeys || 1000,
      ContinuationToken: options.continuationToken,
      Delimiter: options.delimiter,
    });

    const result = await this.client.send(command);

    return {
      objects: (result.Contents || []).map((obj) => ({
        key: obj.Key!,
        size: obj.Size!,
        lastModified: obj.LastModified!,
        etag: obj.ETag?.replace(/"/g, '') || '',
        storageClass: obj.StorageClass,
      })),
      prefixes: (result.CommonPrefixes || []).map((p) => p.Prefix!),
      isTruncated: result.IsTruncated || false,
      continuationToken: result.NextContinuationToken,
    };
  }

  // List all objects (handles pagination)
  async listAll(prefix?: string): Promise<S3Object[]> {
    const objects: S3Object[] = [];
    let continuationToken: string | undefined;

    do {
      const result = await this.list({
        prefix,
        continuationToken,
      });

      objects.push(...result.objects);
      continuationToken = result.continuationToken;
    } while (continuationToken);

    return objects;
  }

  // Copy object
  async copy(
    sourceKey: string,
    destinationKey: string,
    options?: { sourceBucket?: string }
  ): Promise<{ key: string; etag: string }> {
    const sourceBucket = options?.sourceBucket || this.bucket;

    const command = new CopyObjectCommand({
      Bucket: this.bucket,
      Key: destinationKey,
      CopySource: `${sourceBucket}/${sourceKey}`,
    });

    const result = await this.client.send(command);

    return {
      key: destinationKey,
      etag: result.CopyObjectResult?.ETag?.replace(/"/g, '') || '',
    };
  }

  // Move object (copy + delete)
  async move(sourceKey: string, destinationKey: string): Promise<{ key: string; etag: string }> {
    const result = await this.copy(sourceKey, destinationKey);
    await this.delete(sourceKey);
    return result;
  }

  // Generate presigned URL for download
  async getPresignedDownloadUrl(
    key: string,
    options: PresignedUrlOptions = {}
  ): Promise<string> {
    const command = new GetObjectCommand({
      Bucket: this.bucket,
      Key: key,
      ResponseContentType: options.responseContentType,
      ResponseContentDisposition: options.responseContentDisposition,
    });

    return getSignedUrl(this.client, command, {
      expiresIn: options.expiresIn || 3600,
    });
  }

  // Generate presigned URL for upload
  async getPresignedUploadUrl(
    key: string,
    options: PresignedUrlOptions & { contentType?: string } = {}
  ): Promise<string> {
    const command = new PutObjectCommand({
      Bucket: this.bucket,
      Key: key,
      ContentType: options.contentType,
    });

    return getSignedUrl(this.client, command, {
      expiresIn: options.expiresIn || 3600,
    });
  }

  private chunkArray<T>(array: T[], size: number): T[][] {
    const chunks: T[][] = [];
    for (let i = 0; i < array.length; i += size) {
      chunks.push(array.slice(i, i + size));
    }
    return chunks;
  }
}

// Factory function
export function createS3Storage(config: S3Config): S3Storage {
  return new S3Storage(config);
}
```

### Express File Upload API

```typescript
// src/routes/upload.ts
import { Router, Request, Response } from 'express';
import multer from 'multer';
import { createS3Storage } from '../storage/s3-client';
import { v4 as uuidv4 } from 'uuid';
import path from 'path';

const router = Router();

const s3 = createS3Storage({
  region: process.env.AWS_REGION || 'us-east-1',
  bucket: process.env.S3_BUCKET || 'my-bucket',
});

// Configure multer for memory storage
const upload = multer({
  storage: multer.memoryStorage(),
  limits: {
    fileSize: 50 * 1024 * 1024, // 50MB limit
  },
  fileFilter: (req, file, cb) => {
    const allowedTypes = ['image/jpeg', 'image/png', 'image/gif', 'application/pdf'];
    if (allowedTypes.includes(file.mimetype)) {
      cb(null, true);
    } else {
      cb(new Error('Invalid file type'));
    }
  },
});

// Single file upload
router.post('/upload', upload.single('file'), async (req: Request, res: Response) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No file provided' });
    }

    const ext = path.extname(req.file.originalname);
    const key = `uploads/${uuidv4()}${ext}`;

    const result = await s3.upload(key, req.file.buffer, {
      contentType: req.file.mimetype,
      metadata: {
        originalName: req.file.originalname,
        uploadedBy: req.user?.id || 'anonymous',
      },
    });

    res.json({
      success: true,
      file: {
        key: result.key,
        url: result.location,
        originalName: req.file.originalname,
        size: req.file.size,
        contentType: req.file.mimetype,
      },
    });
  } catch (error) {
    console.error('Upload error:', error);
    res.status(500).json({ error: 'Upload failed' });
  }
});

// Multiple file upload
router.post('/upload/multiple', upload.array('files', 10), async (req: Request, res: Response) => {
  try {
    const files = req.files as Express.Multer.File[];

    if (!files || files.length === 0) {
      return res.status(400).json({ error: 'No files provided' });
    }

    const uploads = await Promise.all(
      files.map(async (file) => {
        const ext = path.extname(file.originalname);
        const key = `uploads/${uuidv4()}${ext}`;

        const result = await s3.upload(key, file.buffer, {
          contentType: file.mimetype,
          metadata: {
            originalName: file.originalname,
          },
        });

        return {
          key: result.key,
          url: result.location,
          originalName: file.originalname,
          size: file.size,
        };
      })
    );

    res.json({ success: true, files: uploads });
  } catch (error) {
    console.error('Upload error:', error);
    res.status(500).json({ error: 'Upload failed' });
  }
});

// Get presigned upload URL
router.post('/upload/presigned', async (req: Request, res: Response) => {
  try {
    const { filename, contentType } = req.body;

    if (!filename || !contentType) {
      return res.status(400).json({ error: 'filename and contentType required' });
    }

    const ext = path.extname(filename);
    const key = `uploads/${uuidv4()}${ext}`;

    const url = await s3.getPresignedUploadUrl(key, {
      contentType,
      expiresIn: 3600,
    });

    res.json({
      url,
      key,
      expiresIn: 3600,
    });
  } catch (error) {
    console.error('Presigned URL error:', error);
    res.status(500).json({ error: 'Failed to generate URL' });
  }
});

// Get presigned download URL
router.get('/download/:key(*)', async (req: Request, res: Response) => {
  try {
    const { key } = req.params;

    const exists = await s3.exists(key);
    if (!exists) {
      return res.status(404).json({ error: 'File not found' });
    }

    const url = await s3.getPresignedDownloadUrl(key, {
      expiresIn: 3600,
    });

    res.json({ url, expiresIn: 3600 });
  } catch (error) {
    console.error('Download URL error:', error);
    res.status(500).json({ error: 'Failed to generate URL' });
  }
});

// Delete file
router.delete('/files/:key(*)', async (req: Request, res: Response) => {
  try {
    const { key } = req.params;
    await s3.delete(key);
    res.json({ success: true });
  } catch (error) {
    console.error('Delete error:', error);
    res.status(500).json({ error: 'Delete failed' });
  }
});

// List files
router.get('/files', async (req: Request, res: Response) => {
  try {
    const { prefix, maxKeys } = req.query;

    const result = await s3.list({
      prefix: prefix as string,
      maxKeys: maxKeys ? parseInt(maxKeys as string) : 100,
    });

    res.json(result);
  } catch (error) {
    console.error('List error:', error);
    res.status(500).json({ error: 'List failed' });
  }
});

export default router;
```

### React Upload Component

```typescript
// components/S3Upload.tsx
import React, { useState, useCallback } from 'react';

interface UploadedFile {
  key: string;
  url: string;
  originalName: string;
  size: number;
}

interface UploadProgress {
  loaded: number;
  total: number;
  percentage: number;
}

export function useS3Upload() {
  const [uploading, setUploading] = useState(false);
  const [progress, setProgress] = useState<UploadProgress | null>(null);
  const [error, setError] = useState<string | null>(null);

  // Direct upload through server
  const uploadDirect = useCallback(async (
    file: File,
    onProgress?: (progress: UploadProgress) => void
  ): Promise<UploadedFile> => {
    setUploading(true);
    setError(null);
    setProgress(null);

    try {
      const formData = new FormData();
      formData.append('file', file);

      const xhr = new XMLHttpRequest();

      const uploadPromise = new Promise<UploadedFile>((resolve, reject) => {
        xhr.upload.addEventListener('progress', (e) => {
          if (e.lengthComputable) {
            const prog = {
              loaded: e.loaded,
              total: e.total,
              percentage: Math.round((e.loaded / e.total) * 100),
            };
            setProgress(prog);
            onProgress?.(prog);
          }
        });

        xhr.addEventListener('load', () => {
          if (xhr.status >= 200 && xhr.status < 300) {
            const response = JSON.parse(xhr.responseText);
            resolve(response.file);
          } else {
            reject(new Error('Upload failed'));
          }
        });

        xhr.addEventListener('error', () => reject(new Error('Upload failed')));
        xhr.addEventListener('abort', () => reject(new Error('Upload cancelled')));
      });

      xhr.open('POST', '/api/upload');
      xhr.send(formData);

      return await uploadPromise;
    } catch (err) {
      const message = err instanceof Error ? err.message : 'Upload failed';
      setError(message);
      throw err;
    } finally {
      setUploading(false);
    }
  }, []);

  // Upload using presigned URL
  const uploadPresigned = useCallback(async (
    file: File,
    onProgress?: (progress: UploadProgress) => void
  ): Promise<UploadedFile> => {
    setUploading(true);
    setError(null);
    setProgress(null);

    try {
      // Get presigned URL
      const presignedRes = await fetch('/api/upload/presigned', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          filename: file.name,
          contentType: file.type,
        }),
      });

      if (!presignedRes.ok) {
        throw new Error('Failed to get upload URL');
      }

      const { url, key } = await presignedRes.json();

      // Upload directly to S3
      const xhr = new XMLHttpRequest();

      const uploadPromise = new Promise<void>((resolve, reject) => {
        xhr.upload.addEventListener('progress', (e) => {
          if (e.lengthComputable) {
            const prog = {
              loaded: e.loaded,
              total: e.total,
              percentage: Math.round((e.loaded / e.total) * 100),
            };
            setProgress(prog);
            onProgress?.(prog);
          }
        });

        xhr.addEventListener('load', () => {
          if (xhr.status >= 200 && xhr.status < 300) {
            resolve();
          } else {
            reject(new Error('Upload failed'));
          }
        });

        xhr.addEventListener('error', () => reject(new Error('Upload failed')));
      });

      xhr.open('PUT', url);
      xhr.setRequestHeader('Content-Type', file.type);
      xhr.send(file);

      await uploadPromise;

      return {
        key,
        url: url.split('?')[0],
        originalName: file.name,
        size: file.size,
      };
    } catch (err) {
      const message = err instanceof Error ? err.message : 'Upload failed';
      setError(message);
      throw err;
    } finally {
      setUploading(false);
    }
  }, []);

  return {
    uploadDirect,
    uploadPresigned,
    uploading,
    progress,
    error,
  };
}

// Drag and drop upload component
export function S3Uploader({
  onUpload,
  accept = '*',
  maxSize = 50 * 1024 * 1024,
  usePresigned = true,
}: {
  onUpload: (file: UploadedFile) => void;
  accept?: string;
  maxSize?: number;
  usePresigned?: boolean;
}) {
  const { uploadDirect, uploadPresigned, uploading, progress, error } = useS3Upload();
  const [dragActive, setDragActive] = useState(false);

  const handleUpload = async (file: File) => {
    if (file.size > maxSize) {
      alert(`File too large. Max size: ${Math.round(maxSize / 1024 / 1024)}MB`);
      return;
    }

    try {
      const result = usePresigned
        ? await uploadPresigned(file)
        : await uploadDirect(file);
      onUpload(result);
    } catch (err) {
      console.error('Upload failed:', err);
    }
  };

  const handleDrop = (e: React.DragEvent) => {
    e.preventDefault();
    setDragActive(false);

    const file = e.dataTransfer.files[0];
    if (file) handleUpload(file);
  };

  const handleChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0];
    if (file) handleUpload(file);
  };

  return (
    <div
      className={`upload-zone ${dragActive ? 'active' : ''} ${uploading ? 'uploading' : ''}`}
      onDragOver={(e) => { e.preventDefault(); setDragActive(true); }}
      onDragLeave={() => setDragActive(false)}
      onDrop={handleDrop}
    >
      <input
        type="file"
        accept={accept}
        onChange={handleChange}
        disabled={uploading}
        style={{ display: 'none' }}
        id="file-input"
      />

      <label htmlFor="file-input" className="upload-label">
        {uploading ? (
          <div className="progress">
            <div
              className="progress-bar"
              style={{ width: `${progress?.percentage || 0}%` }}
            />
            <span>{progress?.percentage || 0}%</span>
          </div>
        ) : (
          <span>Drop file here or click to upload</span>
        )}
      </label>

      {error && <p className="error">{error}</p>}
    </div>
  );
}
```

## Python Implementation

```python
# storage/s3_client.py
import boto3
from botocore.config import Config
from botocore.exceptions import ClientError
from typing import Optional, BinaryIO, Dict, List, Any
from dataclasses import dataclass
import mimetypes
import uuid
from pathlib import Path


@dataclass
class S3Object:
    key: str
    size: int
    last_modified: Any
    etag: str
    storage_class: Optional[str] = None


@dataclass
class UploadResult:
    key: str
    etag: str
    location: str


class S3Storage:
    def __init__(
        self,
        bucket: str,
        region: str = 'us-east-1',
        access_key_id: Optional[str] = None,
        secret_access_key: Optional[str] = None,
        endpoint_url: Optional[str] = None,
    ):
        self.bucket = bucket
        self.region = region

        config = Config(
            signature_version='s3v4',
            retries={'max_attempts': 3, 'mode': 'adaptive'}
        )

        session_kwargs = {}
        if access_key_id and secret_access_key:
            session_kwargs['aws_access_key_id'] = access_key_id
            session_kwargs['aws_secret_access_key'] = secret_access_key

        client_kwargs = {
            'region_name': region,
            'config': config,
        }
        if endpoint_url:
            client_kwargs['endpoint_url'] = endpoint_url

        self.client = boto3.client('s3', **client_kwargs, **session_kwargs)
        self.resource = boto3.resource('s3', **client_kwargs, **session_kwargs)

    def upload(
        self,
        key: str,
        body: bytes | BinaryIO,
        content_type: Optional[str] = None,
        metadata: Optional[Dict[str, str]] = None,
        acl: Optional[str] = None,
    ) -> UploadResult:
        """Upload a file to S3."""
        extra_args = {}

        if content_type:
            extra_args['ContentType'] = content_type
        if metadata:
            extra_args['Metadata'] = metadata
        if acl:
            extra_args['ACL'] = acl

        self.client.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=body,
            **extra_args
        )

        return UploadResult(
            key=key,
            etag='',
            location=f'https://{self.bucket}.s3.{self.region}.amazonaws.com/{key}'
        )

    def upload_file(
        self,
        file_path: str,
        key: Optional[str] = None,
        content_type: Optional[str] = None,
    ) -> UploadResult:
        """Upload a file from local filesystem."""
        path = Path(file_path)

        if not key:
            key = f'uploads/{uuid.uuid4()}{path.suffix}'

        if not content_type:
            content_type, _ = mimetypes.guess_type(file_path)

        extra_args = {}
        if content_type:
            extra_args['ContentType'] = content_type

        self.client.upload_file(
            file_path,
            self.bucket,
            key,
            ExtraArgs=extra_args if extra_args else None
        )

        return UploadResult(
            key=key,
            etag='',
            location=f'https://{self.bucket}.s3.{self.region}.amazonaws.com/{key}'
        )

    def download(self, key: str) -> bytes:
        """Download a file as bytes."""
        response = self.client.get_object(Bucket=self.bucket, Key=key)
        return response['Body'].read()

    def download_file(self, key: str, file_path: str) -> None:
        """Download a file to local filesystem."""
        self.client.download_file(self.bucket, key, file_path)

    def delete(self, key: str) -> None:
        """Delete a file."""
        self.client.delete_object(Bucket=self.bucket, Key=key)

    def delete_many(self, keys: List[str]) -> Dict[str, List[str]]:
        """Delete multiple files."""
        deleted = []
        errors = []

        # S3 delete_objects handles up to 1000 keys
        for i in range(0, len(keys), 1000):
            chunk = keys[i:i + 1000]

            response = self.client.delete_objects(
                Bucket=self.bucket,
                Delete={
                    'Objects': [{'Key': k} for k in chunk],
                    'Quiet': False
                }
            )

            deleted.extend([d['Key'] for d in response.get('Deleted', [])])
            errors.extend([e['Key'] for e in response.get('Errors', [])])

        return {'deleted': deleted, 'errors': errors}

    def exists(self, key: str) -> bool:
        """Check if a file exists."""
        try:
            self.client.head_object(Bucket=self.bucket, Key=key)
            return True
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                return False
            raise

    def get_metadata(self, key: str) -> Dict[str, Any]:
        """Get file metadata."""
        response = self.client.head_object(Bucket=self.bucket, Key=key)

        return {
            'content_type': response.get('ContentType'),
            'content_length': response.get('ContentLength'),
            'last_modified': response.get('LastModified'),
            'etag': response.get('ETag', '').strip('"'),
            'metadata': response.get('Metadata', {}),
        }

    def list_objects(
        self,
        prefix: Optional[str] = None,
        max_keys: int = 1000,
        continuation_token: Optional[str] = None,
    ) -> Dict[str, Any]:
        """List objects in bucket."""
        kwargs = {
            'Bucket': self.bucket,
            'MaxKeys': max_keys,
        }

        if prefix:
            kwargs['Prefix'] = prefix
        if continuation_token:
            kwargs['ContinuationToken'] = continuation_token

        response = self.client.list_objects_v2(**kwargs)

        objects = [
            S3Object(
                key=obj['Key'],
                size=obj['Size'],
                last_modified=obj['LastModified'],
                etag=obj['ETag'].strip('"'),
                storage_class=obj.get('StorageClass'),
            )
            for obj in response.get('Contents', [])
        ]

        return {
            'objects': objects,
            'is_truncated': response.get('IsTruncated', False),
            'continuation_token': response.get('NextContinuationToken'),
        }

    def list_all(self, prefix: Optional[str] = None) -> List[S3Object]:
        """List all objects (handles pagination)."""
        objects = []
        continuation_token = None

        while True:
            result = self.list_objects(
                prefix=prefix,
                continuation_token=continuation_token
            )
            objects.extend(result['objects'])

            if not result['is_truncated']:
                break

            continuation_token = result['continuation_token']

        return objects

    def copy(
        self,
        source_key: str,
        dest_key: str,
        source_bucket: Optional[str] = None,
    ) -> UploadResult:
        """Copy an object."""
        source = f'{source_bucket or self.bucket}/{source_key}'

        self.client.copy_object(
            Bucket=self.bucket,
            Key=dest_key,
            CopySource=source
        )

        return UploadResult(
            key=dest_key,
            etag='',
            location=f'https://{self.bucket}.s3.{self.region}.amazonaws.com/{dest_key}'
        )

    def move(self, source_key: str, dest_key: str) -> UploadResult:
        """Move an object (copy + delete)."""
        result = self.copy(source_key, dest_key)
        self.delete(source_key)
        return result

    def generate_presigned_download_url(
        self,
        key: str,
        expires_in: int = 3600,
    ) -> str:
        """Generate presigned URL for download."""
        return self.client.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.bucket, 'Key': key},
            ExpiresIn=expires_in
        )

    def generate_presigned_upload_url(
        self,
        key: str,
        content_type: Optional[str] = None,
        expires_in: int = 3600,
    ) -> str:
        """Generate presigned URL for upload."""
        params = {'Bucket': self.bucket, 'Key': key}

        if content_type:
            params['ContentType'] = content_type

        return self.client.generate_presigned_url(
            'put_object',
            Params=params,
            ExpiresIn=expires_in
        )


# FastAPI integration
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse

app = FastAPI()
s3 = S3Storage(bucket='my-bucket', region='us-east-1')


@app.post('/upload')
async def upload_file(file: UploadFile = File(...)):
    """Upload a file."""
    try:
        content = await file.read()
        key = f'uploads/{uuid.uuid4()}{Path(file.filename).suffix}'

        result = s3.upload(
            key=key,
            body=content,
            content_type=file.content_type
        )

        return {
            'key': result.key,
            'url': result.location,
            'original_name': file.filename,
            'size': len(content),
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post('/upload/presigned')
async def get_presigned_upload_url(filename: str, content_type: str):
    """Get presigned URL for upload."""
    key = f'uploads/{uuid.uuid4()}{Path(filename).suffix}'

    url = s3.generate_presigned_upload_url(
        key=key,
        content_type=content_type
    )

    return {'url': url, 'key': key}


@app.get('/download/{key:path}')
async def get_presigned_download_url(key: str):
    """Get presigned URL for download."""
    if not s3.exists(key):
        raise HTTPException(status_code=404, detail='File not found')

    url = s3.generate_presigned_download_url(key)
    return {'url': url}


@app.delete('/files/{key:path}')
async def delete_file(key: str):
    """Delete a file."""
    s3.delete(key)
    return {'success': True}
```

## CLAUDE.md Integration

```markdown
## S3 Storage Commands

### Upload Operations
- "upload file to S3" - Upload local file
- "create presigned upload URL" - Generate upload URL
- "upload with metadata" - Add custom metadata

### Download Operations
- "download from S3" - Download file
- "create presigned download URL" - Generate download URL
- "stream file from S3" - Stream large files

### Management Operations
- "list S3 objects" - List bucket contents
- "delete S3 file" - Remove file
- "copy S3 object" - Copy between keys
- "move S3 object" - Move/rename file

### Configuration
- "configure S3 bucket" - Set up bucket settings
- "set S3 lifecycle rules" - Configure lifecycle
- "enable S3 versioning" - Turn on versioning
```

## AI Suggestions

1. **"Implement multipart upload for large files"** - Handle files > 5GB with chunked uploads
2. **"Add S3 event notifications"** - Trigger Lambda on upload/delete events
3. **"Implement S3 versioning"** - Enable version history for objects
4. **"Add lifecycle policies"** - Auto-transition to Glacier/delete old files
5. **"Implement S3 replication"** - Cross-region replication for DR
6. **"Add server-side encryption"** - SSE-S3, SSE-KMS, or SSE-C
7. **"Create S3 access points"** - Simplify access management at scale
8. **"Implement S3 Intelligent-Tiering"** - Auto-optimize storage costs
9. **"Add CloudFront CDN integration"** - Accelerate content delivery
10. **"Implement S3 Select"** - Query data without downloading entire objects
